Training configuration:
  - Model: <class 'stable_baselines3.ppo.ppo.PPO'>
  - Processes: 100
  - Iterations: 1000000
  - Base file name: PPO_2025-08-12_12-22-27
  - Policies folder: ./policies
  - Model to load: None

No pre-trained model found, starting training from scratch.
Using cuda device
/home/umberto-francesco-carolini/Desktop/self_balancing_robot_with_DRL/venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
[2K---------------------------------â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m202,500/1,000,000 [0m [ [33m0:00:09[0m < [36m0:00:38[0m , [31m21,150 it/s[0m ]s[0m ]
| rollout/           |          |
|    ep_len_mean     | 14.4     |
|    ep_rew_mean     | -87.5    |
| time/              |          |
|    fps             | 20852    |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 204800   |
---------------------------------
[2K-----------------------------------------â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m407,300/1,000,000 [0m [ [33m0:01:14[0m < [36m0:00:26[0m , [31m23,544 it/s[0m ]s[0m ]
| rollout/                |             |
|    ep_len_mean          | 13.7        |
|    ep_rew_mean          | -87.3       |
| time/                   |             |
|    fps                  | 5463        |
|    iterations           | 2           |
|    time_elapsed         | 74          |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.008312101 |
|    clip_fraction        | 0.0862      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.8        |
|    explained_variance   | 0.000543    |
|    learning_rate        | 0.0003      |
|    loss                 | 61          |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00612    |
|    std                  | 0.971       |
|    value_loss           | 286         |
-----------------------------------------
[2K------------------------------------------â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;2;249;38;114mâ•¸[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m612,600/1,000,000 [0m [ [33m0:02:19[0m < [36m0:00:17[0m , [31m23,447 it/s[0m ]
| rollout/                |              |
|    ep_len_mean          | 15.7         |
|    ep_rew_mean          | -85.5        |
| time/                   |              |
|    fps                  | 4399         |
|    iterations           | 3            |
|    time_elapsed         | 139          |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 0.0128194345 |
|    clip_fraction        | 0.166        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.8         |
|    explained_variance   | 0.614        |
|    learning_rate        | 0.0003       |
|    loss                 | 21.2         |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.0179      |
|    std                  | 0.984        |
|    value_loss           | 51.4         |
------------------------------------------
[2K-----------------------------------------â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[38;5;237mâ•º[0m[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m817,000/1,000,000 [0m [ [33m0:03:24[0m < [36m0:00:08[0m , [31m23,770 it/s[0m ]s[0m ]
| rollout/                |             |
|    ep_len_mean          | 26.6        |
|    ep_rew_mean          | -76.1       |
| time/                   |             |
|    fps                  | 4003        |
|    iterations           | 4           |
|    time_elapsed         | 204         |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.015717844 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.76       |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.5        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0257     |
|    std                  | 0.962       |
|    value_loss           | 65.6        |
-----------------------------------------
[2K-----------------------------------------â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1,022,800/1,000,000 [0m [ [33m0:04:28[0m < [36m0:00:00[0m , [31m23,894 it/s[0m ][31m23,867 it/s[0m ],880 it/s[0m ]
| rollout/                |             |
|    ep_len_mean          | 45.4        |
|    ep_rew_mean          | -57.4       |
| time/                   |             |
|    fps                  | 3797        |
|    iterations           | 5           |
|    time_elapsed         | 269         |
|    total_timesteps      | 1024000     |
| train/                  |             |
|    approx_kl            | 0.013488731 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.7        |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0003      |
|    loss                 | 61.4        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.025      |
|    std                  | 0.933       |
|    value_loss           | 107         |
-----------------------------------------
[2K[35m 100%[0m [38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1,024,000/1,000,000 [0m [ [33m0:04:28[0m < [36m0:00:00[0m , [31m? it/s[0m ]
[?25h
/home/umberto-francesco-carolini/Desktop/self_balancing_robot_with_DRL/venv/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'policies/PPO_2025-08-12_12-22-27' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
/home/umberto-francesco-carolini/Desktop/self_balancing_robot_with_DRL/venv/lib/python3.12/site-packages/stable_baselines3/common/env_checker.py:462: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf. https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html
  warnings.warn(
/home/umberto-francesco-carolini/Desktop/self_balancing_robot_with_DRL/venv/lib/python3.12/site-packages/stable_baselines3/common/env_checker.py:473: UserWarning: Your action space has dtype float64, we recommend using np.float32 to avoid cast errors.
  warnings.warn(
Traceback (most recent call last):
  File "/home/umberto-francesco-carolini/Desktop/self_balancing_robot_with_DRL/train.py", line 217, in <module>
    env.render()
  File "/home/umberto-francesco-carolini/Desktop/self_balancing_robot_with_DRL/venv/lib/python3.12/site-packages/gymnasium/core.py", line 337, in render
    return self.env.render()
           ^^^^^^^^^^^^^^^^^
  File "/home/umberto-francesco-carolini/Desktop/self_balancing_robot_with_DRL/venv/lib/python3.12/site-packages/gymnasium/core.py", line 337, in render
    return self.env.render()
           ^^^^^^^^^^^^^^^^^
  File "/home/umberto-francesco-carolini/Desktop/self_balancing_robot_with_DRL/src/env/self_balancing_robot_env/self_balancing_robot_env.py", line 128, in render
    raise RuntimeError("Viewer is not running. Please reset the environment or start the viewer.")
RuntimeError: Viewer is not running. Please reset the environment or start the viewer.
